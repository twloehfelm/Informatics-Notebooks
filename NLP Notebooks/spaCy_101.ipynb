{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dbbcea-334a-49b0-bb2e-5ab358bd8d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install medspacy > /dev/null\n",
    "!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz > /dev/null\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e81a35-d483-48f0-b92f-db891a53ff1f",
   "metadata": {},
   "source": [
    "RegEx is really useful and very powerful when you are looking for something very specific in a document.\n",
    "\n",
    "What about if you want to extract more general concepts from text? You *could* define a set of really complex RegEx patterns to find all of the possible variations of all of the thigns you are interested in finding, but we saw how complicated it can get even for just one piece of data like the Gleason score.\n",
    "\n",
    "For more general use cases, we need more generalizable tools, and that is where Natural Language Processing (NLP) comes in. NLP is a branch of computer science or subset of artificial intelligence that tries to understand plain spoken or written language, as opposed to non-natural languages (like Python and other programming languages) that have very clearly defined rules and syntax.\n",
    "\n",
    "---\n",
    "\n",
    "You can think of NLP as a series of steps performed in sequence, where each step might depend on the output of the one before, that transforms natural language into structured data. The entire process is referred to as a *pipeline*, and each individual step is referred to as a \"pipe\" component (at least it is in the NLP library we'll be using).\n",
    "\n",
    "---\n",
    "\n",
    "For example, common initial pipes in a radiology-focused NLP pipeline might be to:\n",
    "* ***Sectionize***: split a document up in to *sections* (Technique, Indication, Findings, Impression)\n",
    "* ***Sentencize***: split sections up into *sentences*\n",
    "* ***Tokenize***: split sentences up into *tokens* (words, numbers, punctuation)\n",
    "* ***POS Tagging***: identify parts-of-speech of *tokens*\n",
    "* ***Lemmatize***: transform variants into a common root word (\"be\", \"is\", \"was\", \"were\" == \"be\" after lemmatization)\n",
    "* ***Chunk***: group tokens into phrases based on POS and other token information\n",
    "* ***Dependency Parse***: identify relationships between tokens and phrases - resolve pronouns to their reference token, determine the scope of a negation phrase\n",
    "* ***Named Entity Recognition (NER)***: Detect and label named entities, or categories of tokens that are of interest. In general NLP, these might be \"Person\", \"CompanyName\", \"Country\", \"PhoneNumber\", etc. In radiology NLP, these might be \"Anatomy\", \"Disease\", \"Severity\". Which NEs are recognizable depends on the *Language Model* your pipeline is trained for."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7537a9-6277-44c4-9c74-3b69c11ca35a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "I know it seems really complicated, and it really is. I have a pretty superficial understanding of it, but even just with that you can do some cool stuff.\n",
    "\n",
    "And fortunately, like most things in Python, you don't have to build it from scratch - there are powerful libraries that abstract away all of the complexity for you and lower the barrier to getting started.\n",
    "\n",
    "For Python-based NLP, the dominant framework is called spaCy: https://spacy.io/, and there is a version of it specifically for biomedical NLP called medspaCy: https://spacy.io/universe/project/medspacy\n",
    "\n",
    "We can simply import spacy and medspacy into our programming session and immediately take advantage of their functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7660b654-7a13-49b6-b0ec-bd7dc05b03c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import spacy\n",
    "import medspacy\n",
    "from medspacy.context import ConTextComponent\n",
    "from medspacy.visualization import visualize_dep, visualize_ent\n",
    "from spacy import displacy\n",
    "\n",
    "incl_scispacy_umls_linker = False\n",
    "\n",
    "# //For linking to UMLS - not required just for NER and context annotations, only if the CUIs are required\n",
    "# //Adds significantly to load time and doc processing time\n",
    "if incl_scispacy_umls_linker:\n",
    "    !pip install scispacy\n",
    "    from scispacy.linking import EntityLinker\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8b7b4b-93fd-4631-9103-2d6f8ba63a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an nlp pipeline based on the small version of the \n",
    "# English language core scientific model \"en_core_sci_sm\"\n",
    "nlp = spacy.load(\"en_core_sci_sm\")\n",
    "if incl_scispacy_umls_linker:\n",
    "    # //Add this pipeline component to get UMLS CUIs annotated\n",
    "    nlp.add_pipe(\"scispacy_linker\", config={\"resolve_abbreviations\": True, \"linker_name\": \"umls\"})\n",
    "\n",
    "# Add the ConText pipeline component to our model\n",
    "# This will determine whether NEs are negated, hypothetical, uncertain\n",
    "context = ConTextComponent(nlp)\n",
    "nlp.add_pipe(\"medspacy_context\")\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7126d8-4564-4b01-a268-15faad6f2f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"IMPRESSION: \n",
    "1. Cholelithiasis with positive sonographic Murphy sign. However gallbladder is \n",
    "nondistended without wall thickening. Equivocal for acute cholecystitis.\n",
    "2. Mild intra and extrahepatic ductal dilatation. Correlate with LFTs.\n",
    "If obstructive pattern, consider MRCP.\"\"\"\n",
    "doc = nlp(text)\n",
    "sentences = [{\"idx\":sent_id, \"start\":sent.start_char, \"end\":sent.end_char} for sent_id, sent in enumerate(doc.sents)]\n",
    "results = []\n",
    "for ent in doc.ents:\n",
    "    if incl_scispacy_umls_linker:\n",
    "        try:\n",
    "            cuis = ent._.kb_ents[0][0]\n",
    "        except IndexError:\n",
    "            cuis = None\n",
    "    else:\n",
    "        cuis = None\n",
    "    result = {\n",
    "        \"concept\": ent,\n",
    "        \"start\": ent.start_char,\n",
    "        \"end\": ent.end_char,\n",
    "        \"cui\": cuis,\n",
    "        \"is_negated\": ent._.is_negated,\n",
    "        \"is_uncertain\": ent._.is_uncertain,\n",
    "        \"is_conditional\": ent._.is_hypothetical,\n",
    "        \"is_historic\": ent._.is_historical,\n",
    "        \"subject\": \"family\" if ent._.is_family else \"patient\",\n",
    "        \"sentence\": [sentence[\"idx\"] for sentence in sentences if sentence[\"start\"]<= ent.start_char and sentence[\"end\"]>=ent.end_char][0]\n",
    "    }\n",
    "    results.append(result)\n",
    "    if incl_scispacy_umls_linker:\n",
    "        linker = nlp.get_pipe(\"scispacy_linker\")\n",
    "        for umls_ent in ent._.kb_ents:\n",
    "            print(linker.kb.cui_to_entity[umls_ent[0]])\n",
    "\n",
    "visualize_ent(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02c12f9-6edd-475d-b57d-1bc172914893",
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in results:\n",
    "    if r['concept'].text == \"acute cholecystitis\":\n",
    "        pp.pprint(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846a436d-df8c-4edc-8ce5-0392f626fab2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Try changing \"Equivocal for acute cholecystitis.\" to:\n",
    "* \"Negative for acute cholecystitis.\" => check is_negated\n",
    "* \"Possible acute cholecystitis.\" => check is_uncertain\n",
    "\n",
    "and see if spaCy can recognize that the first variant is negated and the second one is uncertain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698ab9de-1141-4fde-a995-921f2816881c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "It is not too hard to find phrases that are not included in the ConText algorithm's rule set. Here are the phrases it is set up to match by default:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6330cc20-7559-4955-9d94-e5e7020108f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_rules = context.rules\n",
    "context_rules.sort(key=lambda x: x.category)\n",
    "for rule in context_rules:\n",
    "    print(\"%s: %s\" % (rule.category.rjust(20), rule.literal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4e9c33-89ad-4379-9494-5bbbabd8430d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a650c3a4-8115-46d6-8ef3-1c48ee54c80c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "As I mentioned, NLP is much more complex than what we are taking advantage of here. A linguist, or language scientist, could extract much more nuanced information from the NLP annotations. Here's a visualization of the dependency relationships and parts-of-speech of the various tokens in our test document, to give you some idea of what sorts of things you'd have to account for to build an NLP system that truly understands what the text is trying to convey. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e50c383-6bd6-4b47-841a-ab1d2e42e16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {\"compact\": True, \"bg\": \"#19A974\",\n",
    "           \"color\": \"white\", \"font\": \"Avenir\"}\n",
    "displacy.render(doc, style=\"dep\", options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d20429-ed4f-4972-b429-078efa09aed6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
